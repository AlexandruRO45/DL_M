{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Convolutional PyTorch Model to Predict Protein Secondary Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "Your overall goal is to write a Fully Convolutional PyTorch model that can input protein sequence data (often called the Protein Primary Structure ), or additionally using PSSM Profiles to predict the protein secondary structure (H = Helix, E = Extended Sheet, C = Coil symbols).\n",
    "\n",
    "The PDB Database contains the protein structures of over 200,000 proteins. Each has a unique PDB_ID code such as 1A0S (the first one in the training data) which is the structure shown above (sucrose-specific porin of salmonella) which is used to transfer sucrose across the cell membrane of salmonella bacteria which causes food poisoning. The protein has a 3D Structure which shows that most of this protein is extended beta sheet (flat arrows) and coil (random lines).\n",
    "\n",
    "The Data Tab on Kaggle will allow you to browse the available data used for training. You should use this Data Tab to browse through the data so you understand what it is like. You will find a seqs_train.csv file which is a CSV file that gives the PDB_ID (unique identifier) and the SEQUENCE of each protein. You will also find a train.zip file which contains a large collection of 'PDB_ID'_train.csv files containing residue number, amino acid and PSSM profiles for each residue in that particular protein. The labels_train.csv file contains the secondary structure labels for the different training proteins (given as H = Helix, E = Extended Sheet, C = Coil symbols). The seqs_test.csv and test.zip contain similar data for the test sequences for which you need to predict the secondary structure.\n",
    "\n",
    "IN ADDITION - you will also need to submit your Jupyter Notebook that produces these outputs via the Moodle web page.\n",
    "\n",
    "Please see the Moodle course site for further details about this coursework.\n",
    "<br>\n",
    "### Evaluation\n",
    "The evaluation metric is the \"Q3 Accuracy\" which is used for assessing the three states within a protein structure prediction (H = Helix, E = Extended Sheet, C = Coil). <br>\n",
    "\n",
    "### Submission File\n",
    "For each PDB_ID in the test set, you must predict the secondary structure of each residue in that protein. The file should contain a header and have the following format:\n",
    "\n",
    "(So columns give ID consisting of 'PDB_ID', then underscore 'residue number', followed by the predicted secondary structure label of that residue.)\n",
    "\n",
    "ID,STRUCTURE <br>\n",
    "2AIO_1_A_1, C <br>\n",
    "2AIO_1_A_2, C <br>\n",
    "2AIO_1_A_3, C <br>\n",
    "2AIO_1_A_4, H <br>\n",
    "2AIO_1_A_5, H <br>\n",
    "etc. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary libraries and store file paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The code imports necessary libraries for this project.\n",
    "- It checks for the availability of a CUDA-enabled GPU and sets the device accordingly.\n",
    "- It defines file paths for data files (for local use)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from ax.service.ax_client import AxClient, ObjectiveProperties\n",
    "from ax.utils.tutorials.cnn_utils import evaluate, load_mnist, train\n",
    "from ax.utils.notebook.plotting import init_notebook_plotting, render\n",
    "init_notebook_plotting(offline=True)\n",
    "\n",
    "# # Define dtype\n",
    "# dtype = torch.float32  # or torch.float64 depending on your preference\n",
    "\n",
    "# Check if a CUDA-enabled GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"CUDA is available. Using GPU for accelerated computations.\")\n",
    "    print(\"Number of available CUDA devices:\", torch.cuda.device_count())\n",
    "    print(\"GPU device name:\", torch.cuda.get_device_name(0))  # Assuming for only one GPU, index 0\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"CUDA is not available. Using CPU for computations.\")\n",
    "# Display the device being used\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Define store file paths\n",
    "DATA_PATH = \"./data/\"\n",
    "labels_train_path = DATA_PATH + \"labels_train.csv\"\n",
    "sample_path = DATA_PATH + \"sample.csv\"\n",
    "seqs_test_path = DATA_PATH + \"seqs_test.csv\"\n",
    "seqs_train_path = DATA_PATH + \"seqs_train.csv\"\n",
    "train_path = DATA_PATH + \"train\"\n",
    "test_path = DATA_PATH + \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a mapping from amino acid characters to integers\n",
    "\n",
    "To enable model training and assessment, these mappings from amino acid characters to integers for encoding are necessary for converting categorical data into numerical representation: \n",
    "- `sec_struct_mapping`: A dictionary mapping secondary structure labels ('H' for Helix, 'E' for Extended Sheet, 'C' for Coil) to integer labels (0, 1, 2 respectively). Additional mappings can be added if there are more labels.\n",
    "- `amino_acid_mapping`: A dictionary mapping amino acid characters to integer labels. Each amino acid is assigned a unique integer, with additional mappings provided for special cases such as unknown amino acids ('X'), ambiguous cases ('B', 'Z', 'J'), and gap or padding ('-').\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define mappings for secondary structure and amino acids\n",
    "sec_struct_mapping = {\n",
    "    'H': 0,  # Helix\n",
    "    'E': 1,  # Strand\n",
    "    'C': 2   # Coil\n",
    "}\n",
    "\n",
    "amino_acid_mapping = {\n",
    "    'A': 0,   # Alanine\n",
    "    'C': 1,   # Cysteine\n",
    "    'D': 2,   # Aspartic acid\n",
    "    'E': 3,   # Glutamic acid\n",
    "    'F': 4,   # Phenylalanine\n",
    "    'G': 5,   # Glycine\n",
    "    'H': 6,   # Histidine\n",
    "    'I': 7,   # Isoleucine\n",
    "    'K': 8,   # Lysine\n",
    "    'L': 9,   # Leucine\n",
    "    'M': 10,  # Methionine\n",
    "    'N': 11,  # Asparagine\n",
    "    'P': 12,  # Proline\n",
    "    'Q': 13,  # Glutamine\n",
    "    'R': 14,  # Arginine\n",
    "    'S': 15,  # Serine\n",
    "    'T': 16,  # Threonine\n",
    "    'V': 17,  # Valine\n",
    "    'W': 18,  # Tryptophan\n",
    "    'Y': 19,  # Tyrosine\n",
    "    'X': 20,  # Unknown amino acid\n",
    "    'B': 21,  # Asparagine or Aspartic acid\n",
    "    'Z': 22,  # Glutamine or Glutamic acid\n",
    "    'J': 23,  # Leucine or Isoleucine\n",
    "    '-': 24   # Gap or padding\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Dataset for Protein Sequences and PSSM Data\n",
    "\n",
    "Using a custom `Dataset` class allows for efficient management of protein data specifically tailored to your CNN model's needs. It encapsulates data loading, preprocessing, and access logic in a reusable way.\n",
    "### Separate Data Sources:\n",
    "- The code assumes protein sequences are stored in a separate CSV file (`seqs_train.csv` or `seqs_test.csv`) from individual protein data files (likely PSSM profiles) located in a directory (`train`). \n",
    "- This separation can improve code organization and potentially simplify data management if sequences and PSSM data have different update frequencies.\n",
    "### Optional Label Loading: \n",
    "- The code allows loading secondary structure labels from a separate CSV file (`labels_train.csv`) if provided. \n",
    "- This flexibility enables training with or without labels depending on the task (supervised vs. unsupervised learning).\n",
    "### One-Hot Encoding for Sequences:\n",
    "- Converting protein sequences to one-hot encoded representations is a common practice for CNNs. \n",
    "- This transforms categorical amino acids into numerical vectors suitable for the model's computations.\n",
    "### PSSM Normalization:\n",
    "- Normalizing PSSM data can improve model performance by scaling the feature values to a similar range. \n",
    "- The code supports both 'min-max' and 'z-score' normalization methods, allowing you to experiment with different approaches.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset class for handling protein data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, csv_file, train_dir, label_file=None, normalize_method='min-max'):\n",
    "        \"\"\"\n",
    "        Initialize the ProteinDataset.\n",
    "\n",
    "        Parameters:\n",
    "        - csv_file (str): Path to the CSV file containing sequences.\n",
    "        - train_dir (str): Directory containing protein data.\n",
    "        - label_file (str, optional): Path to the CSV file containing labels.\n",
    "        - normalize_method (str, optional): Normalization method for PSSM data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Load the sequences\n",
    "        self.seqs = pd.read_csv(csv_file)\n",
    "\n",
    "        # Load the protein data from the directory\n",
    "        self.protein_data = {}  # Store the protein data in a dictionary\n",
    "        for filename in os.listdir(train_dir):\n",
    "            if filename.endswith(\".csv\"):\n",
    "                protein_id = re.split(r'_train|_test', filename)[0]  # Split the filename to get the protein ID\n",
    "                self.protein_data[protein_id] = pd.read_csv(os.path.join(train_dir, filename))\n",
    "\n",
    "        # Load the labels, if provided\n",
    "        self.labels = pd.read_csv(label_file) if label_file else None\n",
    "\n",
    "        # Amino acid mapping\n",
    "        self.amino_acid_mapping = amino_acid_mapping\n",
    "        self.normalize_method = normalize_method\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the number of sequences in the dataset.\n",
    "\n",
    "        Returns:\n",
    "        - int: Number of sequences.\n",
    "        \"\"\"\n",
    "        return len(self.seqs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get an item from the dataset.\n",
    "\n",
    "        Parameters:\n",
    "        - idx (int): Index of the item.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Tuple containing protein ID, sequence, PSSM, and labels (if available).\n",
    "        \"\"\"\n",
    "\n",
    "        protein_id = self.seqs.iloc[idx]['PDB_ID']  # Get the protein ID\n",
    "        sequence = self.seqs.iloc[idx]['SEQUENCE']  # Get the sequence\n",
    "        encoded_sequence = self.encode_sequence(sequence)  # Encode the sequence\n",
    "        pssm = self.protein_data[protein_id].values  # Assuming you will process PSSM separately\n",
    "        normalized_pssm = self.normalize_pssm(pssm)  # Ensure this is uncommented to use normalized PSSM\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label_seq = self.labels.iloc[idx]['SEC_STRUCT']  # Assuming the label is in the same order as the sequences\n",
    "            label_numeric = [sec_struct_mapping[char] for char in label_seq]  # Convert the label to numeric format\n",
    "            label_tensor = torch.tensor(label_numeric, dtype=torch.long)  # Convert the label to a tensor\n",
    "            # Return protein ID, sequence, PSSM, and label tensor\n",
    "            return (\n",
    "                protein_id,\n",
    "                torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "                torch.tensor(normalized_pssm, dtype=torch.float32),\n",
    "                label_tensor\n",
    "            )\n",
    "        # Return protein ID, sequence, and PSSM\n",
    "        return (\n",
    "            protein_id,\n",
    "            torch.tensor(encoded_sequence, dtype=torch.float32),\n",
    "            torch.tensor(normalized_pssm, dtype=torch.float32)\n",
    "        )\n",
    "\n",
    "    def encode_sequence(self, sequence):\n",
    "        \"\"\"\n",
    "        Encode a sequence into a one-hot encoded vector.\n",
    "\n",
    "        Parameters:\n",
    "        - sequence (str): Sequence to encode.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: One-hot encoded sequence.\n",
    "        \"\"\"\n",
    "\n",
    "        encoded_sequence = np.zeros((len(sequence), len(self.amino_acid_mapping)), dtype=int)\n",
    "        for i, amino_acid in enumerate(sequence):\n",
    "            # Default to 'X' for unknown amino acids\n",
    "            index = self.amino_acid_mapping.get(amino_acid, self.amino_acid_mapping['X'])\n",
    "            encoded_sequence[i, index] = 1\n",
    "        return encoded_sequence\n",
    "\n",
    "    def normalize_pssm(self, pssm):\n",
    "        \"\"\"\n",
    "        Normalize the Position-Specific Scoring Matrix (PSSM).\n",
    "\n",
    "        Parameters:\n",
    "        - pssm (numpy.ndarray): The PSSM data to be normalized.\n",
    "\n",
    "        Returns:\n",
    "        - numpy.ndarray: The normalized PSSM data.\n",
    "        \"\"\"\n",
    "\n",
    "        # Assuming the first two columns are non-numeric\n",
    "        numeric_columns = pssm[:, 2:]  # Adjust this if your numeric data starts from a different column\n",
    "\n",
    "        # Convert to floats\n",
    "        try:\n",
    "            pssm_numeric = numeric_columns.astype(np.float32)  # Float32 is usually sufficient\n",
    "        except ValueError as e:\n",
    "            # Handle or log the error if needed\n",
    "            raise ValueError(f\"Error converting PSSM to float: {e}\")\n",
    "\n",
    "        if self.normalize_method == 'min-max':\n",
    "            # Min-Max normalization\n",
    "            pssm_min = pssm_numeric.min(axis=0)\n",
    "            pssm_max = pssm_numeric.max(axis=0)\n",
    "            # Ensure no division by zero\n",
    "            pssm_range = np.where(pssm_max - pssm_min == 0, 1, pssm_max - pssm_min)\n",
    "            normalized_pssm = (pssm_numeric - pssm_min) / pssm_range\n",
    "        elif self.normalize_method == 'z-score':\n",
    "            # Z-Score normalization\n",
    "            pssm_mean = pssm_numeric.mean(axis=0)\n",
    "            pssm_std = pssm_numeric.std(axis=0)\n",
    "            # Avoid division by zero\n",
    "            pssm_std = np.where(pssm_std == 0, 1, pssm_std)\n",
    "            normalized_pssm = (pssm_numeric - pssm_mean) / pssm_std\n",
    "        else:\n",
    "            # If no normalization method provided, return the original PSSM\n",
    "            normalized_pssm = pssm_numeric\n",
    "\n",
    "        return normalized_pssm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design Choices and Rationale for the ProteinModel Class\n",
    "This class implements a fully convolutional neural network (FCN) model for protein classification. It serves as a fundamental component of the project pipeline for predicting protein secondary structures.\n",
    "\n",
    "### Purpose\n",
    "The purpose of this class is to define the architecture of the FCN model used for protein classification tasks. It consists of convolutional layers followed by a final layer that maps the input features to the number of classes for classification.\n",
    "\n",
    "\n",
    "### Convolutional Architecture:\n",
    "\n",
    "- **Leveraging Sequential Dependencies:** Protein secondary structures often exhibit local dependencies between amino acids. Convolutional layers are well-suited to capture these dependencies as they process the sequence data in a sequential manner, extracting features based on local windows of amino acids.\n",
    "\n",
    "### Specific Layer Choices:\n",
    "\n",
    "- **Multiple Convolutional Layers:** Stacking multiple convolutional layers allows the model to learn increasingly complex hierarchical features from the protein sequence representation. The initial layers capture low-level features like local sequence patterns, while deeper layers learn more abstract and global representations.\n",
    "- **Increasing Channel Depths:** The number of output channels generally increases as we go deeper into the network. This allows the model to learn a richer set of features at each layer, facilitating more complex discriminative power for predicting secondary structures.\n",
    "- **ReLU Activations:** The ReLU (Rectified Linear Unit) activation function is a popular choice for CNNs due to its efficiency and ability to introduce non-linearity. It helps the model learn non-linear relationships between the features extracted by the convolutional layers.\n",
    "- **Final Layer and No Activation:** The final convolutional layer has an output channel dimension equal to the number of predicted secondary structure classes. It typically uses a kernel size of 1 to focus on capturing local dependencies within the feature maps from the previous layer. Since the commonly used CrossEntropyLoss function incorporates a softmax function, a separate activation function is not applied here.\n",
    "\n",
    "### Output Transposition:\n",
    "\n",
    "- **Reshaping for Prediction:** After the final convolutional layer, the output is transposed to have the format `[batch_size, sequence_length, num_classes]`. This ensures the correct output dimensions for predicting a class for each residue in the protein sequence. The model outputs a probability distribution over the class labels for each position in the protein sequence.\n",
    "\n",
    "### Integration with Training Pipeline:\n",
    "\n",
    "- **Initializing the Model:**  An instance of `ProteinModel` would be created within your training script, specifying the number of classes (e.g., 3 for Helix, Extended Sheet, Coil) and the number of input channels (based on your protein sequence representation, like 20 for one-hot encoded amino acids).\n",
    "- **Training Process:** The model would be integrated into your training loop.  The forward pass would be used to compute predictions for protein sequences in each training batch. The loss function (e.g., CrossEntropyLoss) would compare these predictions with the ground truth labels to calculate the error. The optimizer (e.g., Adam) would then use the calculated errors to update the model's weights and biases iteratively during training.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional neural network model for protein classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=3, input_channels=20):\n",
    "        \"\"\"\n",
    "        Initialize the ProteinModel.\n",
    "\n",
    "        Parameters:\n",
    "        - num_classes (int): Number of classes for classification.\n",
    "        - input_channels (int): Number of input channels (e.g., 20 for amino acid one-hot encoding).\n",
    "        \"\"\"\n",
    "\n",
    "        super(ProteinModel, self).__init__()\n",
    "        # Total of 4 convolutional layers\n",
    "        # Define the initial convolutional layer\n",
    "        self.initial_conv = nn.Conv1d(in_channels=input_channels, out_channels=64, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Define three convolutional layers\n",
    "        self.conv1 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final layer that maps to the number of classes\n",
    "        self.final_conv = nn.Conv1d(in_channels=512, out_channels=num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - x (torch.Tensor): Input tensor.\n",
    "\n",
    "        Returns:\n",
    "        - torch.Tensor: Output tensor after passing through the network.\n",
    "        \"\"\"\n",
    "\n",
    "        # Apply convolutional layers with activation functions\n",
    "        x = F.relu(self.initial_conv(x))\n",
    "        \n",
    "        # Apply three convolutional layers to the input 'x', \n",
    "        # each followed by a ReLU activation function to introduce non-linearity.\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "\n",
    "        # Apply final convolutional layer - no activation, as CrossEntropyLoss includes it\n",
    "        x = self.final_conv(x)\n",
    "\n",
    "        # Transpose the output to match [batch_size, sequence_length, num_classes]\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing with Custom Collate Functions\n",
    "\n",
    "\n",
    "### Challenges: with Variable-Length Data:\n",
    "\n",
    "- Our protein sequences and PSSM data might have different lengths, creating difficulties during model training. \n",
    "- Fixed-size data structures are required for efficient vectorized operations within the model.\n",
    "\n",
    "### Solution: Padding and Batching Strategy\n",
    "\n",
    "- Addressing this challenge by defining two custom `collate_fn` functions:\n",
    "    * `collate_fn`: Used for batches without labels (e.g., during pre-training).\n",
    "    * `collate_fn_labels`: Used for batches with labels (e.g., for supervised training).\n",
    "- Both functions perform the following key operations:\n",
    "    * **Unpack Batch Data:** They unpack the input batch, which is a list of tuples containing protein IDs, sequences, and PSSMs (and labels for the function with labels).\n",
    "    * **Pad Sequences and PSSMs:** They utilize the `pad_sequence` function from PyTorch to pad sequences and PSSMs within the batch to a common length. \n",
    "        * This ensures all data points in a batch have the same shape, enabling efficient processing by the CNN model.\n",
    "        * Padding with a specific value (e.g., zeros) allows the model to distinguish between valid data and padding elements.\n",
    "    * **Detaching Gradients:** Sequences and PSSMs are cloned and detached from the computational graph using `clone().detach()`. This can potentially improve memory usage during training by preventing unnecessary gradient calculations for these tensors.\n",
    "\n",
    "### Addressing Labels (if present):\n",
    "\n",
    "- The `collate_fn_labels` function specifically handles label data (secondary structure sequences).\n",
    "    * It checks if labels are present in the batch.\n",
    "    * If labels exist, it applies padding similar to sequences and PSSMs.\n",
    "    * If labels are missing (e.g., during unsupervised pre-training), the function sets the `labels_padded` output to `None`.\n",
    "\n",
    "### Creating a Mask:\n",
    "\n",
    "- Both `collate_fn` functions create a mask tensor based on the original sequence lengths.\n",
    "    * The mask is a binary tensor where 1 indicates a valid sequence position and 0 indicates padding.\n",
    "    * This mask is crucial for the model to focus on relevant protein sequence information during training and avoid getting influenced by padding elements.\n",
    "\n",
    "### Integration with DataLoaders:\n",
    "\n",
    "- These custom `collate_fn` functions are passed as arguments when creating `DataLoader` objects. \n",
    "- The `DataLoader` then utilizes these functions to process individual data points into mini-batches during training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Collate function for processing batches without labels.\n",
    "\n",
    "    Parameters:\n",
    "    - batch (list): List of tuples containing ID, sequences, and PSSMs.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing ID, padded sequences, and padded PSSMs.\n",
    "    \"\"\"\n",
    "\n",
    "    id, sequences, pssms = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    return id, sequences_padded, pssms_padded\n",
    "\n",
    "\n",
    "def collate_fn_labels(batch):\n",
    "    \"\"\"\n",
    "    Collate function for processing batches with labels.\n",
    "\n",
    "    Parameters:\n",
    "    - batch (list): List of tuples containing ID, sequences, PSSMs, and labels.\n",
    "\n",
    "    Returns:\n",
    "    - tuple: Tuple containing padded sequences, padded PSSMs, padded labels, and mask.\n",
    "    \"\"\"\n",
    "\n",
    "    _, sequences, pssms, labels_list = zip(*batch)  # Unzip the batch\n",
    "\n",
    "    # Pad sequences and PSSMs\n",
    "    sequences_padded = pad_sequence([seq.clone().detach() for seq in sequences], batch_first=True)\n",
    "    pssms_padded = pad_sequence([pssm.clone().detach() for pssm in pssms], batch_first=True)\n",
    "\n",
    "    # Handling labels correctly\n",
    "    if labels_list[0] is not None:  # Check if labels exist\n",
    "        labels_padded = pad_sequence([label.clone().detach() for label in labels_list], batch_first=True)\n",
    "    else:\n",
    "        labels_padded = None\n",
    "\n",
    "    # Create a mask based on the original sequence lengths\n",
    "    mask = [torch.ones(len(label), dtype=torch.uint8) for label in labels_list]\n",
    "    mask_padded = pad_sequence(mask, batch_first=True, padding_value=0)  # Assuming padding_value for labels is 0\n",
    "    return sequences_padded, pssms_padded, labels_padded, mask_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training using Hyperparameter Tuning with Ax\n",
    "\n",
    "### Hyperparameters Optimization:\n",
    "\n",
    "- **Integration with Ax:** \n",
    "    * Ax provides a dictionary (`ax_params`) containing training hyperparameters like learning rate and number of epochs.\n",
    "* **Flexibility with Defaults:** \n",
    "    * This allows Ax to explore different values during optimization, while providing default values for missing entries.\n",
    "\n",
    "### Model, Loss, and Optimizer Setup:\n",
    "\n",
    "- **Defining the Model:** An instance of the `ProteinModel` class, specifying the input channel dimension based on your protein sequence representation (e.g., 20 for one-hot encoded amino acids).\n",
    "- **Loss Function:** The `nn.CrossEntropyLoss` function is chosen as it's commonly used for multi-class classification tasks like predicting protein secondary structures. Moving it to the device ensures compatibility with the model.\n",
    "- **Optimizer Selection:** The `torch.optim.Adam` optimizer is a popular choice due to its efficiency and effectiveness. It's configured with the model's parameters and the specified learning rate.\n",
    "\n",
    "### Structured Training Loop:\n",
    "\n",
    "- The core training happens within a loop iterating over a pre-defined number of epochs (`num_epochs`). This allows the model to progressively learn from the data over multiple passes.\n",
    "- **Training Model:** Inside the loop, the model is set to training mode using `model.train()` to activate dropout layers and other training-specific behaviors.\n",
    "- **Tracking Training Statistics:** Initialize variables to track the running loss, correct predictions, and total predictions during the epoch. These metrics provide insights into the training progress.\n",
    "- **Iterating Through Batches:** The function iterates over a data loader named `dataloader_hyper`, which presumably yields mini-batches of pre-processed protein sequence and label data.\n",
    "    * **Processing Each Batch:** For each batch containing padded sequences, PSSMs, labels, and a mask:\n",
    "        * PSSMs are permuted and moved to the device for compatibility with the model's input format.\n",
    "        * Labels are also moved to the device to enable loss calculation on the appropriate hardware.\n",
    "        * Gradients accumulated from previous iterations are cleared using `optimizer.zero_grad()`.\n",
    "        * A forward pass through the model (`model(inputs)`) generates model predictions.\n",
    "        * The model's outputs are reshaped to a format suitable for the `CrossEntropyLoss` function.\n",
    "        * Labels are similarly reshaped to match the output format.\n",
    "        * To handle potential discrepancies in batch sizes across iterations, the minimum batch size is determined and used for slicing both outputs and labels. This ensures compatibility during loss calculation.\n",
    "        * The loss is calculated using the chosen criterion and propagated back with `.backward()` to compute gradients for weight updates.\n",
    "        * The optimizer takes a step (`optimizer.step()`) to update the model's weights based on the calculated gradients, effectively learning from the current batch.\n",
    "        * Training statistics are updated for the current batch (running loss, correct predictions).\n",
    "- **Evaluating Epoch Performance:** After iterating through all batches in an epoch:\n",
    "    * The average epoch loss is calculated by dividing the running loss by the dataset size within the data loader. This provides a measure of the model's overall performance on the training data for that epoch.\n",
    "    * Training accuracy is calculated by dividing the total number of correct predictions by the total number of labels. This metric helps assess how well the model is classifying protein secondary structures.\n",
    "    * Training progress is printed, showing the current epoch number, epoch loss, and training accuracy. This visualization aids in monitoring the training process.\n",
    "\n",
    "### Returning Evaluation Metrics:\n",
    "\n",
    "- After completing all epochs, the function returns a dictionary containing the calculated evaluation metrics (e.g., loss and accuracy). \n",
    "- These metrics are fed back to Ax for hyperparameter optimization, allowing it to identify parameter combinations that lead to better model performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_protein_model(ax_params):\n",
    "    \"\"\"\n",
    "    Train the protein classification model using the given hyperparameters.\n",
    "\n",
    "    Parameters:\n",
    "    - ax_params (dict): Dictionary containing hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "    - dict: Dictionary containing evaluation metrics (loss and accuracy).\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract hyperparameters from Ax parameterization\n",
    "    learning_rate = ax_params.get(\"lr\", 0.001)\n",
    "    num_epochs = ax_params.get(\"num_epochs\", 10)\n",
    "\n",
    "    # Define your model, criterion, optimizer\n",
    "    model = ProteinModel(input_channels=20).to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        for sequences_padded, pssms, labels, mask_padded in dataloader_hyper:\n",
    "            inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "            labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            outputs = outputs.permute(0, 2, 1).contiguous().view(-1, 3)  # Reshape to (batch_size * seq_length, num_classes)\n",
    "            labels = labels.view(-1)\n",
    "            min_batch_size = min(outputs.size(0), labels.size(0))\n",
    "            outputs = outputs[:min_batch_size]\n",
    "            labels = labels[:min_batch_size]\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            _, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "            correct_preds += (predicted == labels).sum().item()\n",
    "            total_preds += labels.numel()\n",
    "\n",
    "        epoch_loss = running_loss / len(dataloader_hyper.dataset)\n",
    "        epoch_acc = correct_preds / total_preds\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.4f}')\n",
    "\n",
    "    # Return the evaluation metric for Ax to optimize (e.g., loss or accuracy)\n",
    "    return {\"loss\": epoch_loss, \"accuracy\": epoch_acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training, Validation, Testing, and Interpretation\n",
    "\n",
    "### Encapsulation for Reusability:\n",
    "\n",
    "- The class encapsulates the training pipeline, promoting reusability across different model architectures. It can train various models for protein secondary structure prediction by simply swapping the model instance within the `ProteinTrainer`.\n",
    "\n",
    "### Modular Components:\n",
    "\n",
    "- The constructor takes the model, criterion (loss function), optimizer, and datasets (train, validation, test) as input. This separation of concerns allows for easy modification of individual components without affecting the entire training pipeline.\n",
    "- Data loaders are created for each dataset using `DataLoader`. These loaders handle batching, shuffling (for training), and potentially applying custom collation functions for specific data processing needs.\n",
    "\n",
    "### Structured Training with Clear Goals:\n",
    "\n",
    "- The `train_model` function orchestrates the training process for a specified number of epochs.\n",
    "    * It maintains separate lists for training and validation metrics (loss and accuracy) to track performance during training.\n",
    "- Inside the training loop:\n",
    "    * The model is set to training mode to activate dropout and other training-specific behaviors.\n",
    "    * Training statistics are initialized for each epoch (loss, correct predictions, total predictions) to monitor progress.\n",
    "    * The function iterates over the training data loader, processing batches containing sequences, PSSMs, labels, and masks.\n",
    "        * PSSMs and labels are moved to the appropriate device (CPU or GPU) for efficient computations.\n",
    "        * Gradients are cleared before each batch using `optimizer.zero_grad()` to avoid accumulating gradients across multiple iterations.\n",
    "        * A forward pass through the model generates predictions.\n",
    "        * The loss is calculated using the criterion and propagated back with `.backward()` for weight updates.\n",
    "        * The optimizer takes a step (`optimizer.step()`) to update the model's weights based on the calculated gradients.\n",
    "        * Training statistics are updated for the current batch.\n",
    "    * After each epoch:\n",
    "        * Average epoch loss and accuracy are calculated for the training data, providing insights into model performance on the training set.\n",
    "        * The model is evaluated on the validation set using `validate_model()`. Validation performance helps assess generalization and avoid overfitting.\n",
    "        * Training progress is printed, showing the current epoch number, training and validation loss/accuracy, allowing you to monitor the training process.\n",
    "\n",
    "### Validation for Generalization Assessment:\n",
    "\n",
    "- The `validate_model` function evaluates the model's performance on the unseen validation set. This helps assess how well the model generalizes to unseen data and avoids overfitting to the training set.\n",
    "- It sets the model to evaluation mode to deactivate dropout layers that might affect predictions.\n",
    "- Similar to training, it iterates over the validation data loader, accumulating loss and accuracy statistics.\n",
    "- It calculates and returns the average validation loss and accuracy, providing a measure of model generalization on unseen data.\n",
    "\n",
    "### Testing for Final Performance Evaluation:\n",
    "\n",
    "- The `test_model` function allows generating predictions on the unseen test set for final evaluation.\n",
    "- It sets the model to evaluation mode.\n",
    "- The function iterates directly over the test dataset, assuming appropriate indexing for accessing data points.\n",
    "    * For each data point (potentially containing PDB ID, sequence, and PSSM):\n",
    "        * The PSSM is converted to a tensor, reshaped, and moved to the device for compatibility with the model.\n",
    "        * A prediction is made using a forward pass through the model.\n",
    "        * The predicted secondary structure label is mapped from the numeric prediction.\n",
    "        * The prediction (residue ID and predicted structure) is appended to a list.\n",
    "- Finally, the predictions are saved to a CSV file using Pandas (assuming it's imported), allowing for further analysis or submission to a competition.\n",
    "\n",
    "### Optional Model Interpretation:\n",
    "\n",
    "- The `interpret_model` function demonstrates using Integrated Gradients from Captum for model interpretation. This is an optional functionality that can provide insights into how the model arrives at its predictions.\n",
    "- It iterates over the data loader (assuming it provides data suitable for interpretation).\n",
    "- For the first batch:\n",
    "    * Model predictions and attributions using Integrated Gradients are calculated.\n",
    "    * The attribution scores for the first sample are printed. These scores highlight which parts of the input PSSM contribute most to the model's predictions, aiding in understanding the model's decision-making process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinTrainer:\n",
    "    \"\"\"\n",
    "    Class for training, validating, testing, and interpreting protein classification models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, criterion, optimizer, train_dataset, val_dataset, test_dataset, batch_size):\n",
    "        \"\"\"\n",
    "        Initialize the ProteinTrainer.\n",
    "\n",
    "        Parameters:\n",
    "        - model (nn.Module): The neural network model.\n",
    "        - criterion: Loss function.\n",
    "        - optimizer: Optimization algorithm.\n",
    "        - train_dataset: Dataset for training.\n",
    "        - val_dataset: Dataset for validation.\n",
    "        - test_dataset: Dataset for testing.\n",
    "        - batch_size (int): Batch size for training.\n",
    "        \"\"\"\n",
    "        self.model = model.to(device)  # Move the model to the device\n",
    "        self.criterion = criterion.to(device)  # Move the loss function to the device\n",
    "        self.optimizer = optimizer\n",
    "        self.train_dataset = train_dataset\n",
    "        self.val_dataset = val_dataset\n",
    "        self.test_dataset = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Create data loaders\n",
    "        self.train_loader = DataLoader(self.train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn_labels)\n",
    "        self.val_loader = DataLoader(self.val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_labels)\n",
    "        self.test_loader = DataLoader(self.test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    def train_model(self, num_epochs):\n",
    "        \"\"\"\n",
    "        Train the model for a specified number of epochs.\n",
    "\n",
    "        Parameters:\n",
    "        - num_epochs (int): Number of epochs for training.\n",
    "        \"\"\"\n",
    "        train_losses = []\n",
    "        train_accuracies = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            self.model.train()  # Set model to training mode\n",
    "            running_loss = 0.0\n",
    "            correct_preds = 0\n",
    "            total_preds = 0\n",
    "\n",
    "            for sequences, pssms, labels, _ in self.train_loader:\n",
    "                inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                self.optimizer.zero_grad() # Clear the gradients\n",
    "\n",
    "                outputs = self.model(inputs) # Forward pass\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels) # Calculate the loss\n",
    "\n",
    "                loss.backward() # Backward pass\n",
    "                self.optimizer.step() # Update weights\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) # Accumulate the loss for the batch\n",
    "\n",
    "                # Calculate training accuracy\n",
    "                _, predicted = torch.max(outputs, 2)  # Get the index of the max log-probability\n",
    "                correct_preds += (predicted == labels).sum().item() # Count the number of correct predictions\n",
    "                total_preds += labels.numel() # Count the total number of predictions\n",
    "\n",
    "            epoch_loss = running_loss / len(self.train_loader.dataset) # Calculate the average loss for the epoch\n",
    "            epoch_acc = correct_preds / total_preds # Calculate the accuracy\n",
    "\n",
    "            # Append training loss and accuracy\n",
    "            train_losses.append(epoch_loss)\n",
    "            train_accuracies.append(epoch_acc)\n",
    "\n",
    "            # Evaluate on validation set\n",
    "            val_loss, val_acc = self.validate_model()\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_acc)\n",
    "\n",
    "            print(f'Epoch {epoch + 1}/{num_epochs}, Train Loss: {epoch_loss:.4f}, Train Accuracy: {epoch_acc:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}')\n",
    "\n",
    "        # Plot loss and accuracy curves\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.subplot(2, 1, 1)\n",
    "        plt.plot(train_losses, label='Train Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.subplot(2, 1, 2)\n",
    "        plt.plot(train_accuracies, label='Train Accuracy')\n",
    "        plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def validate_model(self):\n",
    "        \"\"\"\n",
    "        Validate the model on the validation dataset.\n",
    "\n",
    "        Returns:\n",
    "        - tuple: Validation loss and accuracy.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set model to evaluation mode\n",
    "        running_loss = 0.0\n",
    "        correct_preds = 0\n",
    "        total_preds = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for sequences, pssms, labels, _ in self.val_loader:\n",
    "                inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "                labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "                outputs = self.model(inputs) # Forward pass\n",
    "                loss = self.criterion(outputs.transpose(1, 2), labels) # Calculate the loss\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0) # Accumulate the loss for the batch\n",
    "\n",
    "                # Calculate accuracy\n",
    "                _, predicted = torch.max(outputs, 2) # Get the index of the max log-probability\n",
    "                correct_preds += (predicted == labels).sum().item() # Count the number of correct predictions\n",
    "                total_preds += labels.numel() # Count the total number of predictions\n",
    "\n",
    "        val_loss = running_loss / len(self.val_loader.dataset) # Calculate the average loss for the validation set\n",
    "        val_acc = correct_preds / total_preds # Calculate the accuracy\n",
    "        return val_loss, val_acc\n",
    "\n",
    "    def test_model(self, output_file='./data/submission.csv'):\n",
    "        \"\"\"\n",
    "        Test the model on the test dataset and save predictions to a CSV file.\n",
    "\n",
    "        Parameters:\n",
    "        - output_file (str): Path to save the CSV file with predictions.\n",
    "        \"\"\"\n",
    "        self.model.eval()  # Set the model to evaluation mode\n",
    "        predictions = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(len(self.test_dataset)):  # Iterate directly over the dataset\n",
    "                pdb_id, _, pssm = self.test_dataset[i]  # Assuming the dataset returns PDB_ID, sequence, and PSSM\n",
    "\n",
    "                # Prepare the input tensor; add an extra batch dimension using unsqueeze\n",
    "                input_pssm = pssm.unsqueeze(0).permute(0, 2, 1).to(device)  # Move input to device\n",
    "\n",
    "                # Make a prediction\n",
    "                outputs = self.model(input_pssm) # Forward pass\n",
    "                _, predicted = torch.max(outputs, 2)  # Get the index of max log-probability\n",
    "\n",
    "                # Process the predictions\n",
    "                seq_len = pssm.shape[0]  # Assuming pssm is [features, seq_len]\n",
    "                for j in range(seq_len):\n",
    "                    residue_id = f\"{pdb_id}_{j + 1}\"  # Construct the ID\n",
    "                    structure_label = ['H', 'E', 'C'][predicted[0, j].item()]  # Map numeric predictions to labels\n",
    "                    predictions.append([residue_id, structure_label]) # Append the prediction\n",
    "\n",
    "        # Write predictions to CSV\n",
    "        pd.DataFrame(predictions, columns=['ID', 'STRUCTURE']).to_csv(output_file, index=False)\n",
    "        print(f'Submission file saved to {output_file}')\n",
    "\n",
    "    def interpret_model(self, data_loader):\n",
    "        \"\"\"\n",
    "        Interpret the model predictions using integrated gradients.\n",
    "\n",
    "        Parameters:\n",
    "        - data_loader: Data loader for the dataset.\n",
    "\n",
    "        This method prints the attribution scores for the first sample in the data loader.\n",
    "        \"\"\"\n",
    "        \n",
    "        from captum.attr import IntegratedGradients\n",
    "\n",
    "        ig = IntegratedGradients(self.model)\n",
    "\n",
    "        for sequences, pssms, labels, _ in data_loader:\n",
    "            inputs = pssms.permute(0, 2, 1).to(device)  # Move input to device\n",
    "            labels = labels.to(device)  # Move labels to device\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = self.model(inputs)\n",
    "            _, predicted = torch.max(outputs, 2)\n",
    "\n",
    "            # Compute integrated gradients\n",
    "            attributions, delta = ig.attribute(inputs, target=predicted, return_convergence_delta=True)\n",
    "\n",
    "            # Print the attribution scores for the first sample\n",
    "            print(\"Attribution scores for the first sample:\")\n",
    "            print(attributions[0])\n",
    "\n",
    "            break  # Break after processing the first batch (for demonstration purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path)\n",
    "\n",
    "# Create data loaders\n",
    "dataloader_hyper = DataLoader(train_subset, batch_size=4, shuffle=True, collate_fn=collate_fn_labels)\n",
    "val_dataloader_hyper = DataLoader(val_subset, batch_size=2, shuffle=False, collate_fn=collate_fn_labels)\n",
    "\n",
    "# Create Ax client and experiment\n",
    "ax_client = AxClient()\n",
    "ax_client.create_experiment(\n",
    "    name=\"tune_protein_model\",\n",
    "    parameters=[\n",
    "        {\n",
    "            \"name\": \"lr\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [1e-6, 0.4],\n",
    "            \"value_type\": \"float\",\n",
    "            \"log_scale\": True,\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"num_epochs\",\n",
    "            \"type\": \"range\",\n",
    "            \"bounds\": [10, 100],\n",
    "            \"value_type\": \"int\",\n",
    "        },\n",
    "    ],\n",
    "    objectives={\"accuracy\": ObjectiveProperties(minimize=False)},\n",
    ")\n",
    "\n",
    "# Attach the initial trial\n",
    "ax_client.attach_trial(parameters={\"lr\": 0.01, \"num_epochs\": 20})\n",
    "\n",
    "# Get the parameters and run the initial trial\n",
    "baseline_parameters = ax_client.get_trial_parameters(trial_index=0)\n",
    "print(baseline_parameters)\n",
    "ax_client.complete_trial(trial_index=0, raw_data=train_protein_model(baseline_parameters))\n",
    "\n",
    "# Run additional trials and optimize hyperparameters\n",
    "for i in range(2):\n",
    "    parameters, trial_index = ax_client.get_next_trial()\n",
    "    ax_client.complete_trial(trial_index=trial_index, raw_data=train_protein_model(parameters))\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_parameters, values = ax_client.get_best_parameters()\n",
    "print(\"Best hyperparameters:\", best_parameters)\n",
    "\n",
    "# Create the final model, trainer, and evaluate\n",
    "model = ProteinModel(input_channels=20).to(device)\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_parameters[\"lr\"])\n",
    "\n",
    "trainer = ProteinTrainer(model, criterion, optimizer, train_subset, val_subset, test_dataset, batch_size=32)\n",
    "\n",
    "# Train and evaluate the final model\n",
    "num_epochs = best_parameters[\"num_epochs\"]\n",
    "trainer.train_model(num_epochs)\n",
    "\n",
    "# # Interpret the model using Captum\n",
    "# trainer.interpret_model(trainer.val_loader)\n",
    "\n",
    "# Test the model and save the predictions\n",
    "trainer.validate_model()\n",
    "trainer.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create datasets\n",
    "# dataset = ProteinDataset(csv_file=seqs_train_path, train_dir=train_path, label_file=labels_train_path)\n",
    "# train_size = int(0.8 * len(dataset))\n",
    "# val_size = len(dataset) - train_size\n",
    "# train_subset, val_subset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# test_dataset = ProteinDataset(csv_file=seqs_test_path, train_dir=test_path)\n",
    "\n",
    "# # Create model, loss function, and optimizer\n",
    "# model = ProteinModel()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0)\n",
    "\n",
    "# # Create trainer instance\n",
    "# trainer = ProteinTrainer(model, criterion, optimizer, train_subset, val_subset, test_dataset, batch_size=64)\n",
    "\n",
    "# # Train and evaluate the model\n",
    "# num_epochs = 50\n",
    "# trainer.train_model(num_epochs)\n",
    "# trainer.validate_model()\n",
    "# trainer.test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
